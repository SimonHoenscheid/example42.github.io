I"ª<p>Don‚Äôt reinvent the wheel, use existing Puppet Modules to declare your infrastructure.
But how to keep them in sync with upstream?
How to proceed in case of urgent fixes required?
This weeks <strong>Tip of the week</strong> deals with upstream developed modules, how to store them in your local infrastructure, how to synchronize them and how to add urgent patches, without loosing upstream connectivity.</p>

<h3 id="storing-upstream-modules">Storing upstream modules</h3>

<p>Using upstream modules in your control-repository is easy. You just add the wanted modules to your Puppetfile:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mod 'example42/tp', :latest
mod 'example42/tinydata', :latest
mod 'puppetlabs/concat', '3.0.0' # postgresql requires concat &lt; 3.0.0
mod 'puppetlabs/stdlib', :latest
mod 'puppetlabs/vcsrepo', :latest
mod 'puppetlabs/firewall', :latest
mod 'puppetlabs/aws', :latest
mod 'jdowning/rbenv', :latest
mod 'trlinkin/noop', :latest
mod 'puppetlabs/catalog_preview', :latest
mod 'puppet/archive', :latest
mod 'puppetlabs/inifile', :latest
</code></pre></div></div>

<p>Now your heart of your infrastructure automation must access the internet and fetches modules from puppet forge.
How to proceed if you want to keep modules locally?</p>

<p>In general there are two options:</p>

<ul>
  <li>use a private, local forge</li>
  <li>store modules in local git</li>
</ul>

<h4 id="local-forge">Local forge</h4>

<p>Running a local forge server is easy. You can use <a href="https://forge.puppet.com/unibet/forge_server">unibet/forge_server</a>. Other solutions are <a href="https://pulpproject.org/">Pulp</a> or commercial artifacts systems like <a href="https://jfrog.com/artifactory/">JFrog Artifactory</a> or <a href="https://www.sonatype.com/nexus-repository-sonatype">Sonatype Nexus</a></p>

<p>Major difference between unibet forge server and the other mentioned solutions is, that forge server does not act as proxy to the real forge server.
You just place the modules as tar.gz files into the forge server directory.</p>

<h4 id="local-git">Local GIT</h4>

<p>But how to proceed if you don‚Äôt want to run a commercial artifacts platform or a forge server?
How can you easily and superfast fix issues in existing code?</p>

<p>In this case you want to place the upstream modules in your local git server.</p>

<p>Some enterprise Git solutions allow you to specify an upstream source when creating a new module. Some implementations will even sync periodically from upstream to your local Git repository.</p>

<p>You definitley want a regular sync from upstream to your local working copy.
Within your Puppetfile you refer to the synchronized upstream module by specifying the desired tag:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mod 'puppetlabs/concat',
  :git =&gt; '&lt;user&gt;@&lt;gitserver&gt;:&lt;path&gt;/puppetlabs-concat.git',
  :ref =&gt; '3.0.0' # postgresql requires concat &lt; 3.0.0
</code></pre></div></div>

<h3 id="module-synchronization">Module synchronization</h3>

<p>How do you get synchronization when using git?
First you create the repositories on your local Git server and clone them locally:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone &lt;your localgit url&gt;/&lt;path&gt;/&lt;repo&gt;.git

git clone git@gitserver/puppet7puppetlabs-concat.git
</code></pre></div></div>

<p>Now you switch into the empty directory and add a new remote:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git remote add github https://github.com/puppetlabs/puppetlabs-concat.git
</code></pre></div></div>

<p>From the newly created remote with the name ‚Äúgithub‚Äù we fetch all onjects and non-objects like taks and branches:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git fetch --all
git pull github master
</code></pre></div></div>

<p>Now we have identical code base compared to upstream. We now push all objects and non-objects to our local Git server:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git push origin master --tags
</code></pre></div></div>

<p>Every time when you want to upgrade, you run the last three commands on each of your synchronized Git repositories.
Wait! Manual work, when we do automation? This does not feel good.</p>

<h3 id="multiple-module-synchronization">Multiple module synchronization</h3>

<p>When managing many repositories it becomes error prone and time consuming when doing this in a manual pattern.
Luckily there is help around:</p>

<ul>
  <li><a href="https://android.googlesource.com/tools/repo">repo</a></li>
  <li><a href="https://github.com/dirk-thomas/vcstool">vcstool</a></li>
  <li><a href="https://github.com/vcstools/rosinstall">rosinstall</a></li>
  <li><a href="https://github.com/xolox/python-vcs-repo-mgr">python-vcs-repo-mgr</a></li>
  <li><a href="https://github.com/Masterminds/vcs">go-vcs</a></li>
</ul>

<p>The most simple one - from my point of view - is <a href="https://myrepos.branchable.com/">myrepos</a> a Perl script from Joey Hess.</p>

<p>Within a configuration file we provide the list of modules which we want to manage additionally we provide commands which we want to execute. In the lib section we specify commands with parameters:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># ~/git/.mrconfig
[DEFAULT]

checkout = git clone ssh://&lt;user&gt;@&lt;git server&gt;/&lt;path&gt;/$(basename $MR_REPO).git
pull = git pull --rebase
fetch = git fetch --all
master = if [ $(git branch | grep master) ]; then git checkout master; git reset --hard ;git pull --rebase; fi
prod = if [ $(git branch | grep production) ]; then git checkout production ;git reset --hard ;git pull --rebase; fi

clean = rm -fr vendor .bundle spec/fixtures/modules

remote_update = git checkout master; git fetch --all --prune; git pull github master; git push origin master --tags
lib =
    remote () {
      git remote -v | grep $1 || git remote add github $1 || echo 'remote already added'
    }

# Our control repository
[puppet-control-repo]

## Upstream modules
[puppetlabs-concat]
remote = remote https://github.com/puppetlabs/puppetlabs-concat.git

[puppetlabs-stdlib]
remote = remote https://github.com/puppetlabs/puppetlabs-stdlib

[puppetlabs-inifile]
remote = remote https://github.com/puppetlabs/puppetlabs-inifile
# ...
</code></pre></div></div>

<p>Now you can get your whole repository synced to your local machine with just one command: <code class="highlighter-rouge">mr checkout</code>.
When you return to work, you want to ensure that your local code is up to date. Run: <code class="highlighter-rouge">mr master</code> and <code class="highlighter-rouge">mr prod</code>.
To add the remote you run: <code class="highlighter-rouge">mr remote</code>. Nice side effect: everybody has the same remote set! No more checking which puppet-foo module you had taken initially.</p>

<p>To update the local Git server you run <code class="highlighter-rouge">mr remote_update</code>.</p>

<p>Best option is to have this process running via cron using an application user to push updates.</p>

<h3 id="local-patching-and-remote-pr">Local patching and remote PR</h3>

<p>But how to proceed if you encounter issues with released versions of a module?
Usually you want to create an issue on upstream location (github) and wait for someone to fix it.
Maybe you are even able to provide a PR for the issue.
Buth then you have to wait until upstream creates a new release.</p>

<p>In the menatime you can do the following to your local Git copy of the upstream module:</p>

<ul>
  <li>create a local branch</li>
  <li>add changes to the branch, commit and push them locally</li>
  <li>within your puppetfile, switch to the branch name or the commit id</li>
</ul>

<p>When upstream fixes the issue you just update your local module working copy and switch back to release tags.</p>

<p>Happy hacking,
Martin Alfke</p>
:ET